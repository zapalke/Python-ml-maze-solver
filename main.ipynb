{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maze solver with reinforced learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from collections import defaultdict\n",
    "from time import time\n",
    "import imageio.v2 as imageio\n",
    "from IPython import display\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_actions = {'D':[1,0],'U':[-1,0],'R':[0,1],'L':[0,-1]}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maze generator\n",
    "Function takes maze shape (X,Y), objects to be generated (by default 0-normal, 1- wall) and their probability (80/20) \\\n",
    "It always makes points (0,0) - begining and (X,Y) - end walkable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_maze(X, Y, objects, probability):\n",
    "        if len(objects) != len(probability):\n",
    "            print(f'[Error] Objects and probability have different number of items.')\n",
    "            return None\n",
    "        elif np.sum(probability) != 1.0:\n",
    "            print(f'[Error] Sum of probabilities does not equal 1.0.')\n",
    "            return None\n",
    "        else:\n",
    "            maze = np.random.choice(objects,(X,Y),p=probability)\n",
    "            maze[0,0] = 0\n",
    "            maze[X-1,Y-1] = 0\n",
    "            key_pos_x = random.randrange(1, X-1)\n",
    "            key_pos_y = random.randrange(1, Y-1)\n",
    "            maze[key_pos_x,key_pos_y] = 2\n",
    "            key_pos = (key_pos_x, key_pos_y)\n",
    "            return maze, key_pos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maze class\n",
    "It is responsible for doing everything thats connected directly to maze. \n",
    "\n",
    "### Functions find_allowed_steps, check_move, move_value,\n",
    "After being initialised it generates dictionary of all possible moves. For example \\\n",
    "|----------------------------- \\\n",
    "|-Position: 0,0                 \\\n",
    "|   |- D -> Cost: 1             \\\n",
    "|   |- R -> Cost: 100           \\\n",
    "It is possible to print *allowed_steps* by calling **print_allowed_steps** function.\n",
    "\n",
    "### Updating maze\n",
    "After calling **update_maze** function with choosen action it:\n",
    "1. Updates current robot position\n",
    "   1. Also it checks if its possible to do the action. Its useful when updating maze manually.\n",
    "2. It gets the *reward* of the action\n",
    "3. Finally it appends the move to the *steps* list which includes the history of all steps taken by the robot.\n",
    "Function returns current *robot state* and *reward*.\n",
    "\n",
    "### Reseting maze\n",
    "Calling **return_results_and_reset** returns move history (*steps*) along with *score* of the epoche. \\\n",
    "Then it resets *score*, *steps* and *robot position*.\n",
    "\n",
    "### Printing maze\n",
    "It is possible to call **show_maze** function to visualise current state of maze. \\\n",
    "Function recreates all steps taken of the robot and then prints plot of the maze.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    def __init__(self, maze,color_scale,specials_points={0:1,1:100,2:1}):\n",
    "        self.maze = maze[0]\n",
    "        self.special_points = specials_points       #Dict containing points for each of the objects\n",
    "        self.Y,self.X = [i-1 for i in maze[0].shape]   #Size of maze -1 because we start from (0,0)\n",
    "        self.robot_position = (0,0)                 #Current position of robot\n",
    "        self.steps = []                             #All steps taken by robot\n",
    "        self.score = 0\n",
    "        self.allowed_steps = defaultdict(list)      #List of allowed stapes\n",
    "        self.find_allowed_steps()                   #Construct dict of possible steps along with cost\n",
    "        self.color_scale = color_scale\n",
    "        self.has_key = False\n",
    "        self.key_pos_c = tuple(maze[1])\n",
    "\n",
    "    def check_move(self, y, x, action):\n",
    "        y += allowed_actions[action][0]\n",
    "        x += allowed_actions[action][1]\n",
    "        if x < 0 or y < 0 or x > self.X or y > self.Y:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def move_value(self, y, x, action):\n",
    "        y += allowed_actions[action][0]\n",
    "        x += allowed_actions[action][1]\n",
    "        return self.special_points[self.maze[y,x]]\n",
    "\n",
    "    def check_game_over(self):\n",
    "        if self.robot_position == (self.Y,self.X) and self.has_key == True:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def find_allowed_steps(self):\n",
    "        for y in range(self.Y+1):\n",
    "            for x in range(self.X+1):\n",
    "                for action in allowed_actions:\n",
    "                    if self.check_move(y,x,action):\n",
    "                        self.allowed_steps[(y,x)].append(action)\n",
    "    \n",
    "    def print_allowed_steps(self):\n",
    "        string_size = 30\n",
    "        print('|' + '-'*(string_size-1))\n",
    "        for k, v in self.allowed_steps.items():\n",
    "            string = f'Position: {k[0]},{k[1]}'\n",
    "            for action in v:\n",
    "                string += '\\n'\n",
    "                string += f'|   |- {action} -> Cost: {self.move_value(k[0],k[1],action)}'\n",
    "            print('|-' + string)\n",
    "        print('|' + '-'*(string_size-1))\n",
    "        return self.allowed_steps\n",
    "\n",
    "    def update_maze(self,action):\n",
    "        y, x = self.robot_position\n",
    "        if self.check_move(y, x, action):\n",
    "            reward = self.move_value(y,x,action)\n",
    "            self.score += reward\n",
    "            y += allowed_actions[action][0]\n",
    "            x += allowed_actions[action][1]\n",
    "            self.robot_position = (y,x)\n",
    "            self.steps.append(action)\n",
    "            #print(self.score)\n",
    "            if self.robot_position == self.key_pos_c:\n",
    "                self.has_key = True\n",
    "        else:\n",
    "            print('Move imposible.')\n",
    "        return self.robot_position, reward\n",
    "    \n",
    "    def return_results_and_reset(self):\n",
    "        result_score = self.score\n",
    "        result_steps = self.steps\n",
    "        self.score = 0\n",
    "        self.steps = []\n",
    "        self.robot_position = (0,0)\n",
    "        self.has_key = False\n",
    "        return result_score, result_steps\n",
    "    \n",
    "    \n",
    "    def show_maze(self):\n",
    "        cmap = colors.ListedColormap(self.color_scale)\n",
    "        bounds = np.arange(len(self.color_scale)+1)\n",
    "        norm = colors.BoundaryNorm(bounds,cmap.N)\n",
    "        maze_copy = self.maze.copy()\n",
    "        maze_copy_copy = self.maze.copy()\n",
    "        y, x = (0,0)\n",
    "        for action in self.steps:\n",
    "            maze_copy[(y,x)] = maze_copy_copy[(y,x)] + 5\n",
    "            y += allowed_actions[action][0]\n",
    "            x += allowed_actions[action][1]\n",
    "            maze_copy[(y,x)] = len(self.color_scale)-1\n",
    "        plt.figure(figsize=(X,Y))\n",
    "        plt.figure()\n",
    "        im = plt.imshow(maze_copy,interpolation='none', aspect='equal',\n",
    "                        cmap=cmap,norm=norm)\n",
    "        ax = plt.gca()\n",
    "        # Major ticks\n",
    "        ax.set_xticks(np.arange(0, self.X+1, 1))\n",
    "        ax.set_yticks(np.arange(0, self.Y+1, 1))\n",
    "        # # Minor ticks\n",
    "        ax.set_xticks(np.arange(-.5, self.X+1, 1), minor=True)\n",
    "        ax.set_yticks(np.arange(-.5, self.Y+1, 1), minor=True)\n",
    "        # Gridlines based on minor ticks\n",
    "        ax.grid(which='minor', color='black', linestyle='-', linewidth=1)\n",
    "        # Remove minor ticks\n",
    "        ax.tick_params(which='minor', bottom=False, left=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent class\n",
    "Class responsible for learning robot to solve the maze.\\\n",
    "The goal of the robot is to minimize *reward*, so be as close to *target* (0) as possible.\n",
    "\n",
    "### Expected reward\n",
    "The agent starts with calling **init_reward** function which generates table (*G*) of random expected rewards for each move. The reward are between 0.1 to max(special_points).\n",
    "\n",
    "### Loss function\n",
    "After each epoche agent updates expected reward for each state using this formula:\n",
    "$$ G_{state} = G_{state} + \\alpha(target-G_{state})$$\n",
    "Where $\\alpha$ is learning rate and $target$ is the target reward. So this formula adds some percantage of difference between actual reward and expected reward.\n",
    "\n",
    "### Learning\n",
    "\n",
    "Function **learn** is responsible for updating the values of *G* (basically learning). It knows the target (because we minimize we chose 0)  and updates values using formula above. \n",
    "\n",
    "### Choosing action\n",
    "To choose an action agent firstly decides between exploring and exploiting the maze. \\\n",
    "Exploring makes agent go to random directions to explore rewards in the maze. \\\n",
    "Exploiting means that the agent will choose an action where reward will be the lowest. \\\n",
    "We set the algorythm to explore 20% of the time but we will decrease this number over time as the agent learns rewards in the maze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, states,special_points, alpha=0.15, random_factor=0.2):\n",
    "        self.special_points = special_points\n",
    "        self.state_history = [[(0,0),0]]\n",
    "        self.alpha = alpha\n",
    "        self.random_factor = random_factor\n",
    "        self.G = {}\n",
    "        self.init_rewards(states)\n",
    "        \n",
    "    def init_rewards(self, states):\n",
    "        #Initialize random state table \n",
    "        for i, row in enumerate(states):\n",
    "            for j, col in enumerate(row):\n",
    "                self.G[(i,j)] = np.random.uniform(high=10.0, low=0.1)\n",
    "\n",
    "    def update_state_history(self, state, reward):\n",
    "        self.state_history.append((state, reward))\n",
    "    \n",
    "    def learn(self):\n",
    "        target = 0                                                          # we know the \"ideal\" reward\n",
    "        \n",
    "        for state, reward in reversed(self.state_history):\n",
    "            self.G[state] = self.G[state]+ self.alpha * (target - self.G[state])\n",
    "            target -= reward\n",
    "        self.state_history = []                                             # reset the state_history\n",
    "        self.random_factor -= 10e-6                                      # decrease random_factor\n",
    "\n",
    "\n",
    "    def choose_action(self, state, allowed_moves):\n",
    "        maxG = -10e15\n",
    "        next_move = None\n",
    "        if np.random.random() < self.random_factor:\n",
    "            next_move = np.random.choice(allowed_moves)\n",
    "        else:                                       \n",
    "            for action in allowed_moves:\n",
    "                new_state = tuple([sum(x) for x in zip(state, allowed_actions[action])])\n",
    "                if self.G[new_state] >= maxG:\n",
    "                    next_move = action\n",
    "                    maxG = self.G[new_state]\n",
    "        return next_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_epoche_results(i, t, score, steps_taken, best_score, best_epoch):\n",
    "    #Funtion that prints results of each epoch\n",
    "    string_len = 20\n",
    "    epoche_string = f'Epoche {i}'\n",
    "    epoche_string = epoche_string + \" \"*(string_len-len(epoche_string)-1) + ' | '\n",
    "\n",
    "    score_string = f'Score: {score}'\n",
    "    score_string = score_string + \" \"*(string_len-len(score_string)-1) + ' | '\n",
    "\n",
    "    steps_string = f'Steps: {len(steps_taken)}'\n",
    "    steps_string = steps_string + \" \"*(string_len-len(steps_string)-1) + ' | '\n",
    "\n",
    "    time_string = f'Time: {np.round(t,4)}'\n",
    "    time_string = time_string + \" \"*(string_len-len(time_string)-1) + ' | '\n",
    "\n",
    "    best_score_string = f'Best score: {best_score}'\n",
    "    best_score_string = best_score_string + \" \"*(string_len-len(best_score_string)-1) + ' | '\n",
    "\n",
    "    best_epoch_string = f'Best epoch: {best_epoch}'\n",
    "    best_epoch_string = best_epoch_string + \" \"*(string_len-len(best_epoch_string)-1) + ' | '\n",
    "\n",
    "    print('| '+epoche_string+score_string+steps_string+time_string+best_score_string+best_epoch_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stop_condition(best_result, best_epoch, score, epoch, stop_after = 200):\n",
    "    #Stop learning after given number of iterations without better result\n",
    "    \n",
    "    if best_result > score:\n",
    "        best_result = score\n",
    "        best_epoch = epoch\n",
    "        return False, best_result, best_epoch\n",
    "    elif epoch - best_epoch == stop_after:\n",
    "        return True, best_result, best_epoch\n",
    "    else:\n",
    "        return False, best_result, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recreate_game(steps,maze,color_scale,tick_labels):\n",
    "    if not os.path.exists('img'):\n",
    "        os.makedirs('img')\n",
    "    filenames = []\n",
    "    maze_copy = maze.copy()\n",
    "    maze_copy_copy = maze.copy()\n",
    "    cmap = colors.ListedColormap(color_scale)\n",
    "    bounds = np.arange(len(color_scale)+1)\n",
    "    norm = colors.BoundaryNorm(bounds,cmap.N)\n",
    "    Y, X = maze.shape\n",
    "    y, x = (0,0)\n",
    "    for index, action in enumerate(steps):\n",
    "        maze_copy[(y,x)] = maze_copy_copy[(y,x)] + 5\n",
    "        y += allowed_actions[action][0]\n",
    "        x += allowed_actions[action][1]\n",
    "        maze_copy[(y,x)] = len(color_scale)-1\n",
    "        plt.figure(figsize=(X,Y))\n",
    "        im = plt.imshow(maze_copy,interpolation='none', aspect='equal',\n",
    "                        cmap=cmap,norm=norm)\n",
    "        ax = plt.gca()\n",
    "        # Major ticks\n",
    "        ax.set_xticks(np.arange(0, X, 1))\n",
    "        ax.set_yticks(np.arange(0, Y, 1))\n",
    "        # # Minor ticks\n",
    "        ax.set_xticks(np.arange(-.5, X, 1), minor=True)\n",
    "        ax.set_yticks(np.arange(-.5, Y, 1), minor=True)\n",
    "        # Gridlines based on minor ticks\n",
    "        ax.grid(which='minor', color='black', linestyle='-', linewidth=1)\n",
    "        # Remove minor ticks\n",
    "        ax.tick_params(which='minor', bottom=False, left=False)\n",
    "        #Colorbar\n",
    "        cbar = plt.colorbar(im,shrink=0.75)\n",
    "        cbar.set_ticks(bounds)\n",
    "        cbar.set_ticklabels(tick_labels)\n",
    "        filenames.append(f'{index}.png')\n",
    "        plt.savefig(os.path.join('img',f'{index}.png'))\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gif(filenames,name='result.gif', frame_duration = 0.1):\n",
    "    with imageio.get_writer(name, mode='I', duration = frame_duration) as writer:\n",
    "        for filename in filenames:\n",
    "            image = imageio.imread(os.path.join('img',filename))\n",
    "            writer.append_data(image)\n",
    "\n",
    "    for filename in filenames:\n",
    "        os.remove(os.path.join('img',filename)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, Y = 10, 10\n",
    "X, Y = 15, 15\n",
    "points_path = 2\n",
    "points_wall = 100\n",
    "points_boost = 1\n",
    "points_slowdown = 10\n",
    "key = 2\n",
    "special_points = {0:points_path,1:points_wall, 2:key, 3:points_boost,4:points_slowdown, 5:points_path, 6:points_wall, 7:key, 8:points_boost, 9:points_slowdown}\n",
    "prob = [0.65,0.15,0,0.1,0.1,0,0,0,0,0]\n",
    "color_scale = ['white','red','palegreen','lightblue','orange','grey','darkred','darkgreen','teal','brown','black']\n",
    "tick_labels = [\"\",\"Path\", \"Wall\", 'Key', \"Boost\", \"Slowdown\", 'Walked\\npath','Walked\\nwall', 'Walked\\nKey', 'Walked\\nboost', 'Walked\\nslowdown', 'Current\\nposition']\n",
    "generated_maze = generate_maze(X,Y,list(special_points.keys()),prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_maze = generate_maze(X,Y,list(special_points.keys()),prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = Maze(generated_maze, color_scale,special_points)\n",
    "robot = Agent(maze.maze,special_points, alpha=0.0001, random_factor=0.2)\n",
    "maze.show_maze()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoches = 100000\n",
    "best_result = 0\n",
    "best_epoch = epoches\n",
    "results = {}\n",
    "overall_time = time()\n",
    "for i in range(epoches):\n",
    "    print('| '+('-'*20+'| ')*6)\n",
    "    t = time()\n",
    "    while not maze.check_game_over():    \n",
    "        state = maze.robot_position        \n",
    "        action = robot.choose_action(state, maze.allowed_steps[state])      # Choose an action (explore or exploit)\n",
    "        state, reward = maze.update_maze(action)                            # Update maze state\n",
    "        robot.update_state_history(state, reward)                           # Update robot memory and reward\n",
    "        if len(maze.steps) > (X*Y*2):                                       # If robot cannot reach the end in given time move it to the end\n",
    "            maze.robot_position = (maze.Y, maze.X)\n",
    "            maze.has_key = True\n",
    "    robot.learn()                                                          \n",
    "    score, steps_taken = maze.return_results_and_reset()                    # Return result and reset maze\n",
    "    if best_result == 0:                                                    # Check stop condition\n",
    "        best_result = score\n",
    "        best_epoch = i\n",
    "    else:\n",
    "        stop, best_result, best_epoch = check_stop_condition(best_result, best_epoch, score, i,epoches//5)\n",
    "        if stop:\n",
    "            print(f'Learning stopped. The best score has not been improved for {i-best_epoch} epochs.')\n",
    "            break\n",
    "    results[score] = steps_taken                                            # Update best scores\n",
    "    print_epoche_results(i, time()-t, score,steps_taken, best_result, best_epoch)                    # Print epoche results\n",
    "print(f'Learning was done in {np.round(time()-overall_time,1)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_steps = results[min(results.keys())]\n",
    "print(f'Best score is {best_result} achieved in {len(results[best_result])} steps in epoche {best_epoch}')\n",
    "print(f'Steps taken to win:')\n",
    "for step in best_steps:\n",
    "    print(f'->',end=' ')\n",
    "    print(f'{step}',end=' ')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'result.gif'\n",
    "filenames = recreate_game(best_steps,maze.maze,color_scale,tick_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_gif(filenames,name,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(data=open(name,'rb').read(), format='png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2998579f27ca9b92d13c9e550dcacef04cff8e47eb3c374e44641611705db558"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
